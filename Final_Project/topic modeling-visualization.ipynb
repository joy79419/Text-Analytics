{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Adam_Levine=['Drew_Cole','Jackie_Verna','Mia_Boostrom','Reid_Umstattd']\n",
    "Alicia_Keys=['Johnny_Bliss','Christiana_Danielle','Kelsea_Johnson','Terrence_Cunningham']\n",
    "Blake_Shelton=['Austin_Giorgio','Gary_Edwards','Spensha_Baker','Wilkes']\n",
    "Kelly_Clarkson=['Alexa_Cappelli','D.R._King','Dylan_Hartigan','Tish_Haynes_Keys']\n",
    "experts={'Alicia_Keys':Alicia_Keys,'Adam_Levine':Adam_Levine,'Blake_Shelton':Blake_Shelton,'Kelly_Clarkson':Kelly_Clarkson}\n",
    "for key,value in experts.items():\n",
    "   \n",
    "        \n",
    "    for i in range(len(value)):\n",
    "        data=pd.read_csv(r'C:\\Users\\zgh\\Downloads\\text final\\{}\\{}.csv'.format(key,value[i]))\n",
    "        stop = set(stopwords.words('english'))\n",
    "        exclude = set(string.punctuation) \n",
    "        lemma = WordNetLemmatizer()\n",
    "        datanona=data['commentText'][pd.notna(data['commentText'])]\n",
    "        def clean(doc):\n",
    "            stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "            punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "            other_punc_free=''.join(ch for ch in punc_free if ch!='â€™')\n",
    "            normalized = \" \".join(lemma.lemmatize(word) for word in other_punc_free.split())\n",
    "            replacelove=normalized.replace('love','like')\n",
    "            return replacelove\n",
    "        datacom=datanona.map(clean)\n",
    "        datacomtoken=datacom.map(word_tokenize)\n",
    "        id2word = corpora.Dictionary(datacomtoken)\n",
    "        corpus=[id2word.doc2bow(text) for text in datacomtoken]\n",
    "\n",
    "        NUM_TOPICS = 4\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15)\n",
    "        topics = ldamodel.print_topics(num_words=12)\n",
    "        print (value[i])\n",
    "        vis_data = gensimvis.prepare(lda, corpus, id2word)\n",
    "        pyLDAvis.display(vis_data)\n",
    "        for topic in topics:\n",
    "            print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
