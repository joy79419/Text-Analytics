{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chunhsiangchang/anaconda2/envs/Python3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Blake_Shelton',\n",
       " 'four_performance_word_count.csv',\n",
       " 'Kelly_Clarkson',\n",
       " 'Adam_Levine',\n",
       " 'all_comment_word_count.csv',\n",
       " 'Drew_Cole.csv',\n",
       " 'Alicia_Keys',\n",
       " '.ipynb_checkpoints',\n",
       " 'Text Analytics - Sentiment Scores.ipynb',\n",
       " 'try_w_count.ipynb',\n",
       " 'try_vader.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentors = ['Blake_Shelton', 'Kelly_Clarkson', 'Adam_Levine', 'Alicia_Keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gary_Edwards.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.getcwd()+'/' +'Blake_Shelton')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_pd = pd.DataFrame()\n",
    "comment_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chunhsiangchang/anaconda2/envs/Python3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "first_time = 0\n",
    "for i in mentors:\n",
    "    this_dir = os.getcwd() +'/' + i  + '/'\n",
    "    singers = os.listdir(this_dir)\n",
    "    for j in singers:\n",
    "            if first_time == 0:\n",
    "                comment_pd = pd.read_csv(this_dir + j)\n",
    "                first_time = 1\n",
    "            else:\n",
    "                comment_pd = pd.concat([comment_pd, pd.read_csv(this_dir + j)])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4211"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comment_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_comments = comment_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have youtube video comment data of Wilkes, Austin, Spensha(got saved by the mentor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wilkes = pd.read_csv(\"Wilkes.csv\")\n",
    "\n",
    "austin = pd.read_csv(\"Austin_Giorgio.csv\")\n",
    "\n",
    "spensha = pd.read_csv(\"Spensha_Baker.csv\")\n",
    "\n",
    "gary = pd.read_csv(\"Gary_Edwards.csv\")\n",
    "\n",
    "four_comments = pd.concat([wilkes, austin, spensha, gary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenize_cleaning(comment):\n",
    "    token = word_tokenize(comment)\n",
    "    cleaned_token = []\n",
    "    for word in token:\n",
    "        lower = lowercase= word.lower()\n",
    "        if lower not in punctuation and lower not in stop_words:\n",
    "            cleaned_token.append(word)\n",
    "    return cleaned_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_comments = four_comments[~four_comments[\"commentText\"].isnull()][\"commentText\"].map(text_tokenize_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2894"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_pos_tagging = tokenized_comments.map(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "l = []\n",
    "for comment in comments_pos_tagging:\n",
    "    for word, tag in comment:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            l.append(lemma)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag) \n",
    "            l.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(l).value_counts().to_csv(\"all_comment_word_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have youtube video comment data of Wilkes, Austin, Spensha(got saved by the mentor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilkes = pd.read_csv(\"Wilkes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin = pd.read_csv(\"Austin_Giorgio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spensha = pd.read_csv(\"Spensha_Baker.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For youtube comments, there are \"replies\" to the comments, which failed to be download in a csv format. For \"reply\" the whole row in the data will be NULL and that's why I do try and expect. And those rows will be counted as having 0 sentiment in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_sentiment(comment_series):\n",
    "    fail_count = 0\n",
    "    neu_sentiment_list = []\n",
    "    sentiment_dict= {\"neg\":[], \"neu\":[], \"pos\":[], \"compound\":[]}\n",
    "    for index, comment_message in enumerate(comment_series):\n",
    "        try:\n",
    "            sentiment_score_dict = sid.polarity_scores(comment_message)\n",
    "            for j in sentiment_score_dict:\n",
    "                sentiment_dict[j].append(sentiment_score_dict[j])\n",
    "        except:\n",
    "#            print(\"no sentiment for:\", comment_message)\n",
    "            for j in sentiment_score_dict:\n",
    "                sentiment_dict[j].append(0) \n",
    "            fail_count +=1\n",
    "    \n",
    "        if sentiment_dict[\"compound\"][-1] == 0.0:\n",
    "#            print(\"find\", index)\n",
    "            neu_sentiment_list.append(index)\n",
    "    print(fail_count,\"over\", len(comment_series), \"posts of sentiments were not counted\")\n",
    "    return( pd.DataFrame(sentiment_dict), neu_sentiment_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 over 404 posts of sentiments were not counted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.9716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.155</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.9468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound\n",
       "0  0.024  0.671  0.305    0.9716\n",
       "1  0.155  0.845  0.000   -0.1655\n",
       "2  0.000  0.604  0.396    0.9468"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilkes_sentiment, wilkes_neu_i = get_comment_sentiment(wilkes[\"commentText\"])\n",
    "wilkes_sentiment[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 over 290 posts of sentiments were not counted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.162</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.4588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.185</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.5223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound\n",
       "0  0.162  0.662  0.176    0.1027\n",
       "1  0.000  0.571  0.429    0.4588\n",
       "2  0.185  0.727  0.087   -0.5223"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austin_sentiment, ausitin_neu_i = get_comment_sentiment(austin[\"commentText\"])\n",
    "austin_sentiment[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 over 146 posts of sentiments were not counted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.9118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.5994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neg    neu    pos  compound\n",
       "0  0.0  0.347  0.653    0.9217\n",
       "1  0.0  0.714  0.286    0.9118\n",
       "2  0.0  0.926  0.074    0.5994"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spensha_sentiment, spensha_neu_i = get_comment_sentiment(spensha[\"commentText\"])\n",
    "spensha_sentiment[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,\"['Spensha', 'Wilkes', ' Austin']\")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([spensha_sentiment['compound'], wilkes_sentiment['compound'], austin_sentiment['compound']])\n",
    "plt.xlabel(['Spensha' ,'Wilkes',' Austin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spensha</th>\n",
       "      <th>Wilkes</th>\n",
       "      <th>Austin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.215634</td>\n",
       "      <td>0.209431</td>\n",
       "      <td>0.168207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.454523</td>\n",
       "      <td>0.434821</td>\n",
       "      <td>0.425267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.909700</td>\n",
       "      <td>-0.893400</td>\n",
       "      <td>-0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.624650</td>\n",
       "      <td>0.611550</td>\n",
       "      <td>0.508350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.950100</td>\n",
       "      <td>0.990300</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Spensha      Wilkes      Austin\n",
       "count  146.000000  404.000000  290.000000\n",
       "mean     0.215634    0.209431    0.168207\n",
       "std      0.454523    0.434821    0.425267\n",
       "min     -0.909700   -0.893400   -0.860800\n",
       "25%      0.000000    0.000000    0.000000\n",
       "50%      0.000000    0.000000    0.000000\n",
       "75%      0.624650    0.611550    0.508350\n",
       "max      0.950100    0.990300    0.987500"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Spensha': spensha_sentiment['compound'],\n",
    "              \"Wilkes\":wilkes_sentiment['compound'], \n",
    "              \"Austin\": austin_sentiment['compound']}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second function, I don't include the \"reply\" data, which seems to  be a better way to calculate sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_sentiment_no_reviews(comment_series):\n",
    "    fail_count = 0\n",
    "    neu_sentiment_list = []\n",
    "    sentiment_dict= {\"neg\":[], \"neu\":[], \"pos\":[], \"compound\":[]}\n",
    "    for index, comment_message in enumerate(comment_series):\n",
    "        try:\n",
    "            sentiment_score_dict = sid.polarity_scores(comment_message)\n",
    "            for j in sentiment_score_dict:\n",
    "                sentiment_dict[j].append(sentiment_score_dict[j])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        if sentiment_dict[\"compound\"][-1] == 0.0:\n",
    "#            print(\"find\", index)\n",
    "            neu_sentiment_list.append(index)\n",
    "    print(len(neu_sentiment_list),\"over\", len(comment_series), \"posts of sentiments are neutral\")\n",
    "    return( pd.DataFrame(sentiment_dict), neu_sentiment_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 over 404 posts of sentiments are neutral\n"
     ]
    }
   ],
   "source": [
    "wilkes_sentiment_noR, wilkes_neu_i_noR = get_comment_sentiment_no_reviews(wilkes[\"commentText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 over 290 posts of sentiments are neutral\n"
     ]
    }
   ],
   "source": [
    "austin_sentiment_noR, ausin_neu_i_noR = get_comment_sentiment_no_reviews(austin[\"commentText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 over 146 posts of sentiments are neutral\n"
     ]
    }
   ],
   "source": [
    "spensha_sentiment_noR, spensha_neu_i_noR = get_comment_sentiment_no_reviews(spensha[\"commentText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably the way you can update VADER's dictionary. Hope you can start by improving this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$:': -1.5,\n",
       " '%)': -0.4,\n",
       " '%-)': -1.5,\n",
       " '&-:': -0.4,\n",
       " '&:': -0.7,\n",
       " \"( '}{' )\": 1.6,\n",
       " '(%': -0.9,\n",
       " \"('-:\": 2.2,\n",
       " \"(':\": 2.3,\n",
       " '((-:': 2.1,\n",
       " '(*': 1.1,\n",
       " '(-%': -0.7,\n",
       " '(-*': 1.3,\n",
       " '(-:': 1.6,\n",
       " '(-:0': 2.8,\n",
       " '(-:<': -0.4,\n",
       " '(-:o': 1.5,\n",
       " '(-:O': 1.5,\n",
       " '(-:{': -0.1,\n",
       " '(-:|>*': 1.9,\n",
       " '(-;': 1.3,\n",
       " '(-;|': 2.1,\n",
       " '(8': 2.6,\n",
       " '(:': 2.2,\n",
       " '(:0': 2.4,\n",
       " '(:<': -0.2,\n",
       " '(:o': 2.5,\n",
       " '(:O': 2.5,\n",
       " '(;': 1.1,\n",
       " '(;<': 0.3,\n",
       " '(=': 2.2,\n",
       " '(?:': 2.1,\n",
       " '(^:': 1.5,\n",
       " '(^;': 1.5,\n",
       " '(^;0': 2.0,\n",
       " '(^;o': 1.9,\n",
       " '(o:': 1.6,\n",
       " \")':\": -2.0,\n",
       " \")-':\": -2.1,\n",
       " ')-:': -2.1,\n",
       " ')-:<': -2.2,\n",
       " ')-:{': -2.1,\n",
       " '):': -1.8,\n",
       " '):<': -1.9,\n",
       " '):{': -2.3,\n",
       " ');<': -2.6,\n",
       " '*)': 0.6,\n",
       " '*-)': 0.3,\n",
       " '*-:': 2.1,\n",
       " '*-;': 2.4,\n",
       " '*:': 1.9,\n",
       " '*<|:-)': 1.6,\n",
       " '*\\\\0/*': 2.3,\n",
       " '*^:': 1.6,\n",
       " ',-:': 1.2,\n",
       " \"---'-;-{@\": 2.3,\n",
       " '--<--<@': 2.2,\n",
       " '.-:': -1.2,\n",
       " '..###-:': -1.7,\n",
       " '..###:': -1.9,\n",
       " '/-:': -1.3,\n",
       " '/:': -1.3,\n",
       " '/:<': -1.4,\n",
       " '/=': -0.9,\n",
       " '/^:': -1.0,\n",
       " '/o:': -1.4,\n",
       " '0-8': 0.1,\n",
       " '0-|': -1.2,\n",
       " '0:)': 1.9,\n",
       " '0:-)': 1.4,\n",
       " '0:-3': 1.5,\n",
       " '0:03': 1.9,\n",
       " '0;^)': 1.6,\n",
       " '0_o': -0.3,\n",
       " '10q': 2.1,\n",
       " '1337': 2.1,\n",
       " '143': 3.2,\n",
       " '1432': 2.6,\n",
       " '14aa41': 2.4,\n",
       " '182': -2.9,\n",
       " '187': -3.1,\n",
       " '2g2b4g': 2.8,\n",
       " '2g2bt': -0.1,\n",
       " '2qt': 2.1,\n",
       " '3:(': -2.2,\n",
       " '3:)': 0.5,\n",
       " '3:-(': -2.3,\n",
       " '3:-)': -1.4,\n",
       " '4col': -2.2,\n",
       " '4q': -3.1,\n",
       " '5fs': 1.5,\n",
       " '8)': 1.9,\n",
       " '8-d': 1.7,\n",
       " '8-o': -0.3,\n",
       " '86': -1.6,\n",
       " '8d': 2.9,\n",
       " ':###..': -2.4,\n",
       " ':$': -0.2,\n",
       " ':&': -0.6,\n",
       " \":'(\": -2.2,\n",
       " \":')\": 2.3,\n",
       " \":'-(\": -2.4,\n",
       " \":'-)\": 2.7,\n",
       " ':(': -1.9,\n",
       " ':)': 2.0,\n",
       " ':*': 2.5,\n",
       " ':-###..': -2.5,\n",
       " ':-&': -0.5,\n",
       " ':-(': -1.5,\n",
       " ':-)': 1.3,\n",
       " ':-))': 2.8,\n",
       " ':-*': 1.7,\n",
       " ':-,': 1.1,\n",
       " ':-.': -0.9,\n",
       " ':-/': -1.2,\n",
       " ':-<': -1.5,\n",
       " ':-d': 2.3,\n",
       " ':-D': 2.3,\n",
       " ':-o': 0.1,\n",
       " ':-p': 1.5,\n",
       " ':-[': -1.6,\n",
       " ':-\\\\': -0.9,\n",
       " ':-c': -1.3,\n",
       " ':-|': -0.7,\n",
       " ':-||': -2.5,\n",
       " ':-Þ': 0.9,\n",
       " ':/': -1.4,\n",
       " ':3': 2.3,\n",
       " ':<': -2.1,\n",
       " ':>': 2.1,\n",
       " ':?)': 1.3,\n",
       " ':?c': -1.6,\n",
       " ':@': -2.5,\n",
       " ':d': 2.3,\n",
       " ':D': 2.3,\n",
       " ':l': -1.7,\n",
       " ':o': -0.4,\n",
       " ':p': 1.4,\n",
       " ':s': -1.2,\n",
       " ':[': -2.0,\n",
       " ':\\\\': -1.3,\n",
       " ':]': 2.2,\n",
       " ':^)': 2.1,\n",
       " ':^*': 2.6,\n",
       " ':^/': -1.2,\n",
       " ':^\\\\': -1.0,\n",
       " ':^|': -1.0,\n",
       " ':c': -2.1,\n",
       " ':c)': 2.0,\n",
       " ':o)': 2.1,\n",
       " ':o/': -1.4,\n",
       " ':o\\\\': -1.1,\n",
       " ':o|': -0.6,\n",
       " ':{': -1.9,\n",
       " ':|': -0.4,\n",
       " ':}': 2.1,\n",
       " ':Þ': 1.1,\n",
       " ';)': 0.9,\n",
       " ';-)': 1.0,\n",
       " ';-*': 2.2,\n",
       " ';-]': 0.7,\n",
       " ';d': 0.8,\n",
       " ';D': 0.8,\n",
       " ';]': 0.6,\n",
       " ';^)': 1.4,\n",
       " '</3': -3.0,\n",
       " '<3': 1.9,\n",
       " '<:': 2.1,\n",
       " '<:-|': -1.4,\n",
       " '=)': 2.2,\n",
       " '=-3': 2.0,\n",
       " '=-d': 2.4,\n",
       " '=-D': 2.4,\n",
       " '=/': -1.4,\n",
       " '=3': 2.1,\n",
       " '=d': 2.3,\n",
       " '=D': 2.3,\n",
       " '=l': -1.2,\n",
       " '=\\\\': -1.2,\n",
       " '=]': 1.6,\n",
       " '=p': 1.3,\n",
       " '=|': -0.8,\n",
       " '>-:': -2.0,\n",
       " '>.<': -1.3,\n",
       " '>:': -2.1,\n",
       " '>:(': -2.7,\n",
       " '>:)': 0.4,\n",
       " '>:-(': -2.7,\n",
       " '>:-)': -0.4,\n",
       " '>:/': -1.6,\n",
       " '>:o': -1.2,\n",
       " '>:p': 1.0,\n",
       " '>:[': -2.1,\n",
       " '>:\\\\': -1.7,\n",
       " '>;(': -2.9,\n",
       " '>;)': 0.1,\n",
       " '>_>^': 2.1,\n",
       " '@:': -2.1,\n",
       " '@>-->--': 2.1,\n",
       " \"@}-;-'---\": 2.2,\n",
       " 'aas': 2.5,\n",
       " 'aayf': 2.7,\n",
       " 'afu': -2.9,\n",
       " 'alol': 2.8,\n",
       " 'ambw': 2.9,\n",
       " 'aml': 3.4,\n",
       " 'atab': -1.9,\n",
       " 'awol': -1.3,\n",
       " 'ayc': 0.2,\n",
       " 'ayor': -1.2,\n",
       " 'aug-00': 0.3,\n",
       " 'bfd': -2.7,\n",
       " 'bfe': -2.6,\n",
       " 'bff': 2.9,\n",
       " 'bffn': 1.0,\n",
       " 'bl': 2.3,\n",
       " 'bsod': -2.2,\n",
       " 'btd': -2.1,\n",
       " 'btdt': -0.1,\n",
       " 'bz': 0.4,\n",
       " 'b^d': 2.6,\n",
       " 'cwot': -2.3,\n",
       " \"d-':\": -2.5,\n",
       " 'd8': -3.2,\n",
       " 'd:': 1.2,\n",
       " 'd:<': -3.2,\n",
       " 'd;': -2.9,\n",
       " 'd=': 1.5,\n",
       " 'doa': -2.3,\n",
       " 'dx': -3.0,\n",
       " 'ez': 1.5,\n",
       " 'fav': 2.0,\n",
       " 'fcol': -1.8,\n",
       " 'ff': 1.8,\n",
       " 'ffs': -2.8,\n",
       " 'fkm': -2.4,\n",
       " 'foaf': 1.8,\n",
       " 'ftw': 2.0,\n",
       " 'fu': -3.7,\n",
       " 'fubar': -3.0,\n",
       " 'fwb': 2.5,\n",
       " 'fyi': 0.8,\n",
       " 'fysa': 0.4,\n",
       " 'g1': 1.4,\n",
       " 'gg': 1.2,\n",
       " 'gga': 1.7,\n",
       " 'gigo': -0.6,\n",
       " 'gj': 2.0,\n",
       " 'gl': 1.3,\n",
       " 'gla': 2.5,\n",
       " 'gn': 1.2,\n",
       " 'gr8': 2.7,\n",
       " 'grrr': -0.4,\n",
       " 'gt': 1.1,\n",
       " 'h&k': 2.3,\n",
       " 'hagd': 2.2,\n",
       " 'hagn': 2.2,\n",
       " 'hago': 1.2,\n",
       " 'hak': 1.9,\n",
       " 'hand': 2.2,\n",
       " 'hho1/2k': 1.4,\n",
       " 'hhoj': 2.0,\n",
       " 'hhok': 0.9,\n",
       " 'hugz': 2.0,\n",
       " 'hi5': 1.9,\n",
       " 'idk': -0.4,\n",
       " 'ijs': 0.7,\n",
       " 'ilu': 3.4,\n",
       " 'iluaaf': 2.7,\n",
       " 'ily': 3.4,\n",
       " 'ily2': 2.6,\n",
       " 'iou': 0.7,\n",
       " 'iyq': 2.3,\n",
       " 'j/j': 2.0,\n",
       " 'j/k': 1.6,\n",
       " 'j/p': 1.4,\n",
       " 'j/t': -0.2,\n",
       " 'j/w': 1.0,\n",
       " 'j4f': 1.4,\n",
       " 'j4g': 1.7,\n",
       " 'jho': 0.8,\n",
       " 'jhomf': 1.0,\n",
       " 'jj': 1.0,\n",
       " 'jk': 0.9,\n",
       " 'jp': 0.8,\n",
       " 'jt': 0.9,\n",
       " 'jw': 1.6,\n",
       " 'jealz': -1.2,\n",
       " 'k4y': 2.3,\n",
       " 'kfy': 2.3,\n",
       " 'kia': -3.2,\n",
       " 'kk': 1.5,\n",
       " 'kmuf': 2.2,\n",
       " 'l': 2.0,\n",
       " 'l&r': 2.2,\n",
       " 'laoj': 1.3,\n",
       " 'lmao': 2.9,\n",
       " 'lmbao': 1.8,\n",
       " 'lmfao': 2.5,\n",
       " 'lmso': 2.7,\n",
       " 'lol': 1.8,\n",
       " 'lolz': 2.7,\n",
       " 'lts': 1.6,\n",
       " 'ly': 2.6,\n",
       " 'ly4e': 2.7,\n",
       " 'lya': 3.3,\n",
       " 'lyb': 3.0,\n",
       " 'lyl': 3.1,\n",
       " 'lylab': 2.7,\n",
       " 'lylas': 2.6,\n",
       " 'lylb': 1.6,\n",
       " 'm8': 1.4,\n",
       " 'mia': -1.2,\n",
       " 'mml': 2.0,\n",
       " 'mofo': -2.4,\n",
       " 'muah': 2.3,\n",
       " 'mubar': -1.0,\n",
       " 'musm': 0.9,\n",
       " 'mwah': 2.5,\n",
       " 'n1': 1.9,\n",
       " 'nbd': 1.3,\n",
       " 'nbif': -0.5,\n",
       " 'nfc': -2.7,\n",
       " 'nfw': -2.4,\n",
       " 'nh': 2.2,\n",
       " 'nimby': -0.8,\n",
       " 'nimjd': -0.7,\n",
       " 'nimq': -0.2,\n",
       " 'nimy': -1.4,\n",
       " 'nitl': -1.5,\n",
       " 'nme': -2.1,\n",
       " 'noyb': -0.7,\n",
       " 'np': 1.4,\n",
       " 'ntmu': 1.4,\n",
       " 'o-8': -0.5,\n",
       " 'o-:': -0.3,\n",
       " 'o-|': -1.1,\n",
       " 'o.o': -0.8,\n",
       " 'O.o': -0.6,\n",
       " 'o.O': -0.6,\n",
       " 'o:': -0.2,\n",
       " 'o:)': 1.5,\n",
       " 'o:-)': 2.0,\n",
       " 'o:-3': 2.2,\n",
       " 'o:3': 2.3,\n",
       " 'o:<': -0.3,\n",
       " 'o;^)': 1.6,\n",
       " 'ok': 1.2,\n",
       " 'o_o': -0.5,\n",
       " 'O_o': -0.5,\n",
       " 'o_O': -0.5,\n",
       " 'pita': -2.4,\n",
       " 'pls': 0.3,\n",
       " 'plz': 0.3,\n",
       " 'pmbi': 0.8,\n",
       " 'pmfji': 0.3,\n",
       " 'pmji': 0.7,\n",
       " 'po': -2.6,\n",
       " 'ptl': 2.6,\n",
       " 'pu': -1.1,\n",
       " 'qq': -2.2,\n",
       " 'qt': 1.8,\n",
       " 'r&r': 2.4,\n",
       " 'rofl': 2.7,\n",
       " 'roflmao': 2.5,\n",
       " 'rotfl': 2.6,\n",
       " 'rotflmao': 2.8,\n",
       " 'rotflmfao': 2.5,\n",
       " 'rotflol': 3.0,\n",
       " 'rotgl': 2.9,\n",
       " 'rotglmao': 1.8,\n",
       " 's:': -1.1,\n",
       " 'sapfu': -1.1,\n",
       " 'sete': 2.8,\n",
       " 'sfete': 2.7,\n",
       " 'sgtm': 2.4,\n",
       " 'slap': 0.6,\n",
       " 'slaw': 2.1,\n",
       " 'smh': -1.3,\n",
       " 'snafu': -2.5,\n",
       " 'sob': -1.0,\n",
       " 'swak': 2.3,\n",
       " 'tgif': 2.3,\n",
       " 'thks': 1.4,\n",
       " 'thx': 1.5,\n",
       " 'tia': 2.3,\n",
       " 'tmi': -0.3,\n",
       " 'tnx': 1.1,\n",
       " 'true': 1.8,\n",
       " 'tx': 1.5,\n",
       " 'txs': 1.1,\n",
       " 'ty': 1.6,\n",
       " 'tyvm': 2.5,\n",
       " 'urw': 1.9,\n",
       " 'vbg': 2.1,\n",
       " 'vbs': 3.1,\n",
       " 'vip': 2.3,\n",
       " 'vwd': 2.6,\n",
       " 'vwp': 2.1,\n",
       " 'wag': -0.2,\n",
       " 'wd': 2.7,\n",
       " 'wilco': 0.9,\n",
       " 'wp': 1.0,\n",
       " 'wtf': -2.8,\n",
       " 'wtg': 2.1,\n",
       " 'wth': -2.4,\n",
       " 'x-d': 2.6,\n",
       " 'x-p': 1.7,\n",
       " 'xd': 2.8,\n",
       " 'xlnt': 3.0,\n",
       " 'xoxo': 3.0,\n",
       " 'xoxozzz': 2.3,\n",
       " 'xp': 1.6,\n",
       " 'xqzt': 1.6,\n",
       " 'xtc': 0.8,\n",
       " 'yolo': 1.1,\n",
       " 'yoyo': 0.4,\n",
       " 'yvw': 1.6,\n",
       " 'yw': 1.8,\n",
       " 'ywia': 2.5,\n",
       " 'zzz': -1.2,\n",
       " '[-;': 0.5,\n",
       " '[:': 1.3,\n",
       " '[;': 1.0,\n",
       " '[=': 1.7,\n",
       " '\\\\-:': -1.0,\n",
       " '\\\\:': -1.0,\n",
       " '\\\\:<': -1.7,\n",
       " '\\\\=': -1.1,\n",
       " '\\\\^:': -1.3,\n",
       " '\\\\o/': 2.2,\n",
       " '\\\\o:': -1.2,\n",
       " ']-:': -2.1,\n",
       " ']:': -1.6,\n",
       " ']:<': -2.5,\n",
       " '^<_<': 1.4,\n",
       " '^urs': -2.8,\n",
       " 'abandon': -1.9,\n",
       " 'abandoned': -2.0,\n",
       " 'abandoner': -1.9,\n",
       " 'abandoners': -1.9,\n",
       " 'abandoning': -1.6,\n",
       " 'abandonment': -2.4,\n",
       " 'abandonments': -1.7,\n",
       " 'abandons': -1.3,\n",
       " 'abducted': -2.3,\n",
       " 'abduction': -2.8,\n",
       " 'abductions': -2.0,\n",
       " 'abhor': -2.0,\n",
       " 'abhorred': -2.4,\n",
       " 'abhorrent': -3.1,\n",
       " 'abhors': -2.9,\n",
       " 'abilities': 1.0,\n",
       " 'ability': 1.3,\n",
       " 'aboard': 0.1,\n",
       " 'absentee': -1.1,\n",
       " 'absentees': -0.8,\n",
       " 'absolve': 1.2,\n",
       " 'absolved': 1.5,\n",
       " 'absolves': 1.3,\n",
       " 'absolving': 1.6,\n",
       " 'abuse': -3.2,\n",
       " 'abused': -2.3,\n",
       " 'abuser': -2.6,\n",
       " 'abusers': -2.6,\n",
       " 'abuses': -2.6,\n",
       " 'abusing': -2.0,\n",
       " 'abusive': -3.2,\n",
       " 'abusively': -2.8,\n",
       " 'abusiveness': -2.5,\n",
       " 'abusivenesses': -3.0,\n",
       " 'accept': 1.6,\n",
       " 'acceptabilities': 1.6,\n",
       " 'acceptability': 1.1,\n",
       " 'acceptable': 1.3,\n",
       " 'acceptableness': 1.3,\n",
       " 'acceptably': 1.5,\n",
       " 'acceptance': 2.0,\n",
       " 'acceptances': 1.7,\n",
       " 'acceptant': 1.6,\n",
       " 'acceptation': 1.3,\n",
       " 'acceptations': 0.9,\n",
       " 'accepted': 1.1,\n",
       " 'accepting': 1.6,\n",
       " 'accepts': 1.3,\n",
       " 'accident': -2.1,\n",
       " 'accidental': -0.3,\n",
       " 'accidentally': -1.4,\n",
       " 'accidents': -1.3,\n",
       " 'accomplish': 1.8,\n",
       " 'accomplished': 1.9,\n",
       " 'accomplishes': 1.7,\n",
       " 'accusation': -1.0,\n",
       " 'accusations': -1.3,\n",
       " 'accuse': -0.8,\n",
       " 'accused': -1.2,\n",
       " 'accuses': -1.4,\n",
       " 'accusing': -0.7,\n",
       " 'ache': -1.6,\n",
       " 'ached': -1.6,\n",
       " 'aches': -1.0,\n",
       " 'achievable': 1.3,\n",
       " 'aching': -2.2,\n",
       " 'acquit': 0.8,\n",
       " 'acquits': 0.1,\n",
       " 'acquitted': 1.0,\n",
       " 'acquitting': 1.3,\n",
       " 'acrimonious': -1.7,\n",
       " 'active': 1.7,\n",
       " 'actively': 1.3,\n",
       " 'activeness': 0.6,\n",
       " 'activenesses': 0.8,\n",
       " 'actives': 1.1,\n",
       " 'adequate': 0.9,\n",
       " 'admirability': 2.4,\n",
       " 'admirable': 2.6,\n",
       " 'admirableness': 2.2,\n",
       " 'admirably': 2.5,\n",
       " 'admiral': 1.3,\n",
       " 'admirals': 1.5,\n",
       " 'admiralties': 1.6,\n",
       " 'admiralty': 1.2,\n",
       " 'admiration': 2.5,\n",
       " 'admirations': 1.6,\n",
       " 'admire': 2.1,\n",
       " 'admired': 2.3,\n",
       " 'admirer': 1.8,\n",
       " 'admirers': 1.7,\n",
       " 'admires': 1.5,\n",
       " 'admiring': 1.6,\n",
       " 'admiringly': 2.3,\n",
       " 'admit': 0.8,\n",
       " 'admits': 1.2,\n",
       " 'admitted': 0.4,\n",
       " 'admonished': -1.9,\n",
       " 'adopt': 0.7,\n",
       " 'adopts': 0.7,\n",
       " 'adorability': 2.2,\n",
       " 'adorable': 2.2,\n",
       " 'adorableness': 2.5,\n",
       " 'adorably': 2.1,\n",
       " 'adoration': 2.9,\n",
       " 'adorations': 2.2,\n",
       " 'adore': 2.6,\n",
       " 'adored': 1.8,\n",
       " 'adorer': 1.7,\n",
       " 'adorers': 2.1,\n",
       " 'adores': 1.6,\n",
       " 'adoring': 2.6,\n",
       " 'adoringly': 2.4,\n",
       " 'adorn': 0.9,\n",
       " 'adorned': 0.8,\n",
       " 'adorner': 1.3,\n",
       " 'adorners': 0.9,\n",
       " 'adorning': 1.0,\n",
       " 'adornment': 1.3,\n",
       " 'adornments': 0.8,\n",
       " 'adorns': 0.5,\n",
       " 'advanced': 1.0,\n",
       " 'advantage': 1.0,\n",
       " 'advantaged': 1.4,\n",
       " 'advantageous': 1.5,\n",
       " 'advantageously': 1.9,\n",
       " 'advantageousness': 1.6,\n",
       " 'advantages': 1.5,\n",
       " 'advantaging': 1.6,\n",
       " 'adventure': 1.3,\n",
       " 'adventured': 1.3,\n",
       " 'adventurer': 1.2,\n",
       " 'adventurers': 0.9,\n",
       " 'adventures': 1.4,\n",
       " 'adventuresome': 1.7,\n",
       " 'adventuresomeness': 1.3,\n",
       " 'adventuress': 0.8,\n",
       " 'adventuresses': 1.4,\n",
       " 'adventuring': 2.3,\n",
       " 'adventurism': 1.5,\n",
       " 'adventurist': 1.4,\n",
       " 'adventuristic': 1.7,\n",
       " 'adventurists': 1.2,\n",
       " 'adventurous': 1.4,\n",
       " 'adventurously': 1.3,\n",
       " 'adventurousness': 1.8,\n",
       " 'adversarial': -1.5,\n",
       " 'adversaries': -1.0,\n",
       " 'adversary': -0.8,\n",
       " 'adversative': -1.2,\n",
       " 'adversatively': -0.1,\n",
       " 'adversatives': -1.0,\n",
       " 'adverse': -1.5,\n",
       " 'adversely': -0.8,\n",
       " 'adverseness': -0.6,\n",
       " 'adversities': -1.5,\n",
       " 'adversity': -1.8,\n",
       " 'affected': -0.6,\n",
       " 'affection': 2.4,\n",
       " 'affectional': 1.9,\n",
       " 'affectionally': 1.5,\n",
       " 'affectionate': 1.9,\n",
       " 'affectionately': 2.2,\n",
       " 'affectioned': 1.8,\n",
       " 'affectionless': -2.0,\n",
       " 'affections': 1.5,\n",
       " 'afflicted': -1.5,\n",
       " 'affronted': 0.2,\n",
       " 'aggravate': -2.5,\n",
       " 'aggravated': -1.9,\n",
       " 'aggravates': -1.9,\n",
       " 'aggravating': -1.2,\n",
       " 'aggress': -1.3,\n",
       " 'aggressed': -1.4,\n",
       " 'aggresses': -0.5,\n",
       " 'aggressing': -0.6,\n",
       " 'aggression': -1.2,\n",
       " 'aggressions': -1.3,\n",
       " 'aggressive': -0.6,\n",
       " 'aggressively': -1.3,\n",
       " 'aggressiveness': -1.8,\n",
       " 'aggressivities': -1.4,\n",
       " 'aggressivity': -0.6,\n",
       " 'aggressor': -0.8,\n",
       " 'aggressors': -0.9,\n",
       " 'aghast': -1.9,\n",
       " 'agitate': -1.7,\n",
       " 'agitated': -2.0,\n",
       " 'agitatedly': -1.6,\n",
       " 'agitates': -1.4,\n",
       " 'agitating': -1.8,\n",
       " 'agitation': -1.0,\n",
       " 'agitational': -1.2,\n",
       " 'agitations': -1.3,\n",
       " 'agitative': -1.3,\n",
       " 'agitato': -0.1,\n",
       " 'agitator': -1.4,\n",
       " 'agitators': -2.1,\n",
       " 'agog': 1.9,\n",
       " 'agonise': -2.1,\n",
       " 'agonised': -2.3,\n",
       " 'agonises': -2.4,\n",
       " 'agonising': -1.5,\n",
       " 'agonize': -2.3,\n",
       " 'agonized': -2.2,\n",
       " 'agonizes': -2.3,\n",
       " 'agonizing': -2.7,\n",
       " 'agonizingly': -2.3,\n",
       " 'agony': -1.8,\n",
       " 'agree': 1.5,\n",
       " 'agreeability': 1.9,\n",
       " 'agreeable': 1.8,\n",
       " 'agreeableness': 1.8,\n",
       " 'agreeablenesses': 1.3,\n",
       " 'agreeably': 1.6,\n",
       " 'agreed': 1.1,\n",
       " 'agreeing': 1.4,\n",
       " 'agreement': 2.2,\n",
       " 'agreements': 1.1,\n",
       " 'agrees': 0.8,\n",
       " 'alarm': -1.4,\n",
       " 'alarmed': -1.4,\n",
       " 'alarming': -0.5,\n",
       " 'alarmingly': -2.6,\n",
       " 'alarmism': -0.3,\n",
       " 'alarmists': -1.1,\n",
       " 'alarms': -1.1,\n",
       " 'alas': -1.1,\n",
       " 'alert': 1.2,\n",
       " 'alienation': -1.1,\n",
       " 'alive': 1.6,\n",
       " 'allergic': -1.2,\n",
       " 'allow': 0.9,\n",
       " 'alone': -1.0,\n",
       " 'alright': 1.0,\n",
       " 'amaze': 2.5,\n",
       " 'amazed': 2.2,\n",
       " 'amazedly': 2.1,\n",
       " 'amazement': 2.5,\n",
       " 'amazements': 2.2,\n",
       " 'amazes': 2.2,\n",
       " 'amazing': 2.8,\n",
       " 'amazon': 0.7,\n",
       " 'amazonite': 0.2,\n",
       " 'amazons': -0.1,\n",
       " 'amazonstone': 1.0,\n",
       " 'amazonstones': 0.2,\n",
       " 'ambitious': 2.1,\n",
       " 'ambivalent': 0.5,\n",
       " 'amor': 3.0,\n",
       " 'amoral': -1.6,\n",
       " 'amoralism': -0.7,\n",
       " 'amoralisms': -0.7,\n",
       " 'amoralities': -1.2,\n",
       " 'amorality': -1.5,\n",
       " 'amorally': -1.0,\n",
       " 'amoretti': 0.2,\n",
       " 'amoretto': 0.6,\n",
       " 'amorettos': 0.3,\n",
       " 'amorino': 1.2,\n",
       " 'amorist': 1.6,\n",
       " 'amoristic': 1.0,\n",
       " 'amorists': 0.1,\n",
       " 'amoroso': 2.3,\n",
       " 'amorous': 1.8,\n",
       " 'amorously': 2.3,\n",
       " 'amorousness': 2.0,\n",
       " 'amorphous': -0.2,\n",
       " 'amorphously': 0.1,\n",
       " 'amorphousness': 0.3,\n",
       " 'amort': -2.1,\n",
       " 'amortise': 0.5,\n",
       " 'amortised': -0.2,\n",
       " 'amortises': 0.1,\n",
       " 'amortizable': 0.5,\n",
       " 'amortization': 0.6,\n",
       " 'amortizations': 0.2,\n",
       " 'amortize': -0.1,\n",
       " 'amortized': 0.8,\n",
       " 'amortizes': 0.6,\n",
       " 'amortizing': 0.8,\n",
       " 'amusable': 0.7,\n",
       " 'amuse': 1.7,\n",
       " 'amused': 1.8,\n",
       " 'amusedly': 2.2,\n",
       " 'amusement': 1.5,\n",
       " 'amusements': 1.5,\n",
       " 'amuser': 1.1,\n",
       " 'amusers': 1.3,\n",
       " 'amuses': 1.7,\n",
       " 'amusia': 0.3,\n",
       " 'amusias': -0.4,\n",
       " 'amusing': 1.6,\n",
       " 'amusingly': 0.8,\n",
       " 'amusingness': 1.8,\n",
       " 'amusive': 1.7,\n",
       " 'anger': -2.7,\n",
       " 'angered': -2.3,\n",
       " 'angering': -2.2,\n",
       " 'angerly': -1.9,\n",
       " 'angers': -2.3,\n",
       " 'angrier': -2.3,\n",
       " 'angriest': -3.1,\n",
       " 'angrily': -1.8,\n",
       " 'angriness': -1.7,\n",
       " 'angry': -2.3,\n",
       " 'anguish': -2.9,\n",
       " 'anguished': -1.8,\n",
       " 'anguishes': -2.1,\n",
       " 'anguishing': -2.7,\n",
       " 'animosity': -1.9,\n",
       " 'annoy': -1.9,\n",
       " 'annoyance': -1.3,\n",
       " 'annoyances': -1.8,\n",
       " 'annoyed': -1.6,\n",
       " 'annoyer': -2.2,\n",
       " 'annoyers': -1.5,\n",
       " 'annoying': -1.7,\n",
       " 'annoys': -1.8,\n",
       " 'antagonism': -1.9,\n",
       " 'antagonisms': -1.2,\n",
       " 'antagonist': -1.9,\n",
       " 'antagonistic': -1.7,\n",
       " 'antagonistically': -2.2,\n",
       " 'antagonists': -1.7,\n",
       " 'antagonize': -2.0,\n",
       " 'antagonized': -1.4,\n",
       " 'antagonizes': -0.5,\n",
       " 'antagonizing': -2.7,\n",
       " 'anti': -1.3,\n",
       " 'anticipation': 0.4,\n",
       " 'anxieties': -0.6,\n",
       " 'anxiety': -0.7,\n",
       " 'anxious': -1.0,\n",
       " 'anxiously': -0.9,\n",
       " 'anxiousness': -1.0,\n",
       " 'aok': 2.0,\n",
       " 'apathetic': -1.2,\n",
       " 'apathetically': -0.4,\n",
       " 'apathies': -0.6,\n",
       " 'apathy': -1.2,\n",
       " 'apeshit': -0.9,\n",
       " 'apocalyptic': -3.4,\n",
       " 'apologise': 1.6,\n",
       " 'apologised': 0.4,\n",
       " 'apologises': 0.8,\n",
       " 'apologising': 0.2,\n",
       " 'apologize': 0.4,\n",
       " 'apologized': 1.3,\n",
       " 'apologizes': 1.5,\n",
       " 'apologizing': -0.3,\n",
       " 'apology': 0.2,\n",
       " 'appall': -2.4,\n",
       " 'appalled': -2.0,\n",
       " 'appalling': -1.5,\n",
       " 'appallingly': -2.0,\n",
       " 'appalls': -1.9,\n",
       " 'appease': 1.1,\n",
       " 'appeased': 0.9,\n",
       " 'appeases': 0.9,\n",
       " 'appeasing': 1.0,\n",
       " 'applaud': 2.0,\n",
       " 'applauded': 1.5,\n",
       " 'applauding': 2.1,\n",
       " 'applauds': 1.4,\n",
       " 'applause': 1.8,\n",
       " 'appreciate': 1.7,\n",
       " 'appreciated': 2.3,\n",
       " 'appreciates': 2.3,\n",
       " 'appreciating': 1.9,\n",
       " 'appreciation': 2.3,\n",
       " 'appreciations': 1.7,\n",
       " 'appreciative': 2.6,\n",
       " 'appreciatively': 1.8,\n",
       " 'appreciativeness': 1.6,\n",
       " 'appreciator': 2.6,\n",
       " 'appreciators': 1.5,\n",
       " 'appreciatory': 1.7,\n",
       " 'apprehensible': 1.1,\n",
       " 'apprehensibly': -0.2,\n",
       " 'apprehension': -2.1,\n",
       " 'apprehensions': -0.9,\n",
       " 'apprehensively': -0.3,\n",
       " 'apprehensiveness': -0.7,\n",
       " 'approval': 2.1,\n",
       " 'approved': 1.8,\n",
       " 'approves': 1.7,\n",
       " 'ardent': 2.1,\n",
       " 'arguable': -1.0,\n",
       " 'arguably': -1.0,\n",
       " 'argue': -1.4,\n",
       " 'argued': -1.5,\n",
       " 'arguer': -1.6,\n",
       " 'arguers': -1.4,\n",
       " 'argues': -1.6,\n",
       " 'arguing': -2.0,\n",
       " 'argument': -1.5,\n",
       " 'argumentative': -1.5,\n",
       " 'argumentatively': -1.8,\n",
       " 'argumentive': -1.5,\n",
       " 'arguments': -1.7,\n",
       " 'arrest': -1.4,\n",
       " 'arrested': -2.1,\n",
       " 'arrests': -1.9,\n",
       " 'arrogance': -2.4,\n",
       " 'arrogances': -1.9,\n",
       " 'arrogant': -2.2,\n",
       " 'arrogantly': -1.8,\n",
       " 'ashamed': -2.1,\n",
       " 'ashamedly': -1.7,\n",
       " 'ass': -2.5,\n",
       " 'assassination': -2.9,\n",
       " 'assassinations': -2.7,\n",
       " 'assault': -2.8,\n",
       " 'assaulted': -2.4,\n",
       " 'assaulting': -2.3,\n",
       " 'assaultive': -2.8,\n",
       " 'assaults': -2.5,\n",
       " 'asset': 1.5,\n",
       " 'assets': 0.7,\n",
       " 'assfucking': -2.5,\n",
       " 'assholes': -2.8,\n",
       " 'assurance': 1.4,\n",
       " 'assurances': 1.4,\n",
       " 'assure': 1.4,\n",
       " 'assured': 1.5,\n",
       " 'assuredly': 1.6,\n",
       " 'assuredness': 1.4,\n",
       " 'assurer': 0.9,\n",
       " 'assurers': 1.1,\n",
       " 'assures': 1.3,\n",
       " 'assurgent': 1.3,\n",
       " 'assuring': 1.6,\n",
       " 'assuror': 0.5,\n",
       " 'assurors': 0.7,\n",
       " 'astonished': 1.6,\n",
       " 'astound': 1.7,\n",
       " 'astounded': 1.8,\n",
       " 'astounding': 1.8,\n",
       " 'astoundingly': 2.1,\n",
       " 'astounds': 2.1,\n",
       " 'attachment': 1.2,\n",
       " 'attachments': 1.1,\n",
       " 'attack': -2.1,\n",
       " 'attacked': -2.0,\n",
       " 'attacker': -2.7,\n",
       " 'attackers': -2.7,\n",
       " 'attacking': -2.0,\n",
       " 'attacks': -1.9,\n",
       " 'attract': 1.5,\n",
       " 'attractancy': 0.9,\n",
       " 'attractant': 1.3,\n",
       " 'attractants': 1.4,\n",
       " 'attracted': 1.8,\n",
       " 'attracting': 2.1,\n",
       " 'attraction': 2.0,\n",
       " 'attractions': 1.8,\n",
       " 'attractive': 1.9,\n",
       " 'attractively': 2.2,\n",
       " 'attractiveness': 1.8,\n",
       " 'attractivenesses': 2.1,\n",
       " 'attractor': 1.2,\n",
       " 'attractors': 1.2,\n",
       " 'attracts': 1.7,\n",
       " 'audacious': 0.9,\n",
       " 'authority': 0.3,\n",
       " 'aversion': -1.9,\n",
       " 'aversions': -1.1,\n",
       " 'aversive': -1.6,\n",
       " 'aversively': -0.8,\n",
       " 'avert': -0.7,\n",
       " 'averted': -0.3,\n",
       " 'averts': -0.4,\n",
       " 'avid': 1.2,\n",
       " 'avoid': -1.2,\n",
       " 'avoidance': -1.7,\n",
       " 'avoidances': -1.1,\n",
       " 'avoided': -1.4,\n",
       " 'avoider': -1.8,\n",
       " 'avoiders': -1.4,\n",
       " 'avoiding': -1.4,\n",
       " 'avoids': -0.7,\n",
       " 'await': 0.4,\n",
       " 'awaited': -0.1,\n",
       " 'awaits': 0.3,\n",
       " 'award': 2.5,\n",
       " 'awardable': 2.4,\n",
       " 'awarded': 1.7,\n",
       " 'awardee': 1.8,\n",
       " 'awardees': 1.2,\n",
       " 'awarder': 0.9,\n",
       " 'awarders': 1.3,\n",
       " 'awarding': 1.9,\n",
       " 'awards': 2.0,\n",
       " 'awesome': 3.1,\n",
       " 'awful': -2.0,\n",
       " 'awkward': -0.6,\n",
       " 'awkwardly': -1.3,\n",
       " 'awkwardness': -0.7,\n",
       " 'axe': -0.4,\n",
       " 'axed': -1.3,\n",
       " 'backed': 0.1,\n",
       " 'backing': 0.1,\n",
       " 'backs': -0.2,\n",
       " 'bad': -2.5,\n",
       " 'badass': -0.6,\n",
       " 'badly': -2.1,\n",
       " 'bailout': -0.4,\n",
       " 'bamboozle': -1.5,\n",
       " 'bamboozled': -1.5,\n",
       " 'bamboozles': -1.5,\n",
       " 'ban': -2.6,\n",
       " 'banish': -1.9,\n",
       " 'bankrupt': -2.6,\n",
       " 'bankster': -2.1,\n",
       " 'banned': -2.0,\n",
       " 'bargain': 0.8,\n",
       " 'barrier': -0.5,\n",
       " 'bashful': -0.1,\n",
       " 'bashfully': 0.2,\n",
       " 'bashfulness': -0.8,\n",
       " 'bastard': -2.5,\n",
       " 'bastardies': -1.8,\n",
       " 'bastardise': -2.1,\n",
       " 'bastardised': -2.3,\n",
       " 'bastardises': -2.3,\n",
       " 'bastardising': -2.6,\n",
       " 'bastardization': -2.4,\n",
       " 'bastardizations': -2.1,\n",
       " 'bastardize': -2.4,\n",
       " 'bastardized': -2.0,\n",
       " 'bastardizes': -1.8,\n",
       " 'bastardizing': -2.3,\n",
       " 'bastardly': -2.7,\n",
       " 'bastards': -3.0,\n",
       " 'bastardy': -2.7,\n",
       " 'battle': -1.6,\n",
       " 'battled': -1.2,\n",
       " 'battlefield': -1.6,\n",
       " 'battlefields': -0.9,\n",
       " 'battlefront': -1.2,\n",
       " 'battlefronts': -0.8,\n",
       " 'battleground': -1.7,\n",
       " 'battlegrounds': -0.6,\n",
       " 'battlement': -0.4,\n",
       " 'battlements': -0.4,\n",
       " 'battler': -0.8,\n",
       " 'battlers': -0.2,\n",
       " 'battles': -1.6,\n",
       " 'battleship': -0.1,\n",
       " 'battleships': -0.5,\n",
       " 'battlewagon': -0.3,\n",
       " 'battlewagons': -0.5,\n",
       " 'battling': -1.1,\n",
       " 'beaten': -1.8,\n",
       " 'beatific': 1.8,\n",
       " 'beating': -2.0,\n",
       " 'beaut': 1.6,\n",
       " 'beauteous': 2.5,\n",
       " 'beauteously': 2.6,\n",
       " 'beauteousness': 2.7,\n",
       " 'beautician': 1.2,\n",
       " 'beauticians': 0.4,\n",
       " ...}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "sia.lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spensha</th>\n",
       "      <th>Wilkes</th>\n",
       "      <th>Austin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.297005</td>\n",
       "      <td>0.272935</td>\n",
       "      <td>0.227944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.510779</td>\n",
       "      <td>0.478745</td>\n",
       "      <td>0.481353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.909700</td>\n",
       "      <td>-0.893400</td>\n",
       "      <td>-0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.445050</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.701725</td>\n",
       "      <td>0.669550</td>\n",
       "      <td>0.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.950100</td>\n",
       "      <td>0.990300</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Spensha      Wilkes      Austin\n",
       "count  106.000000  310.000000  214.000000\n",
       "mean     0.297005    0.272935    0.227944\n",
       "std      0.510779    0.478745    0.481353\n",
       "min     -0.909700   -0.893400   -0.860800\n",
       "25%      0.000000    0.000000    0.000000\n",
       "50%      0.445050    0.296000    0.248300\n",
       "75%      0.701725    0.669550    0.636900\n",
       "max      0.950100    0.990300    0.987500"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Spensha': spensha_sentiment_noR['compound'],\n",
    "              \"Wilkes\":wilkes_sentiment_noR['compound'], \n",
    "              \"Austin\": austin_sentiment_noR['compound']}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    this sound exactly like the original song, don...\n",
       "1                   This was cute. Nothing spectacular\n",
       "2    this is honestly the song he needed, he wasnt ...\n",
       "3      we love your voice Austin Giorgio,don't give up\n",
       "4                                    Ugh that suit 😍😍😍\n",
       "5                    Why he sounds like Michael Buble?\n",
       "6                                 Damn this was boring\n",
       "7    I really think this was an awful song choice. ...\n",
       "8    I think this song choice wasn't a wise one. It...\n",
       "9    This man is my decepcion off the playoffs i th...\n",
       "Name: commentText, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austin[\"commentText\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg         0.4830\n",
       "neu         0.5170\n",
       "pos         0.0000\n",
       "compound   -0.4215\n",
       "Name: 4, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austin_sentiment.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     I loved and love this version that you made of...\n",
       "1     He lost maybe cause of that but he shouldn’t have\n",
       "2     I absolutely love Wilkes performances...lookin...\n",
       "3     Okay so Wilkes, this performance was Stupid go...\n",
       "4                                      OMG that was lit\n",
       "5                                This version was sick!\n",
       "6     Love your voice and thought sure you would be ...\n",
       "7                                     bring him back...\n",
       "8                                           r o b b e d\n",
       "9                                                   😍😍😍\n",
       "10    Was the mic drop intentionally or an accident?...\n",
       "11                                                  NaN\n",
       "12    I want to download Wilkes singing this song. \\...\n",
       "13                                                  NaN\n",
       "14                                              Amazing\n",
       "Name: commentText, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilkes[\"commentText\"][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
