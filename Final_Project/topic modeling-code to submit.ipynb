{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 topics on all singers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johnny_Bliss\n",
      " \n",
      "(0, '0.016*\"really\" + 0.015*\"go\" + 0.013*\"get\" + 0.011*\"top\" + 0.009*\"playoff\" + 0.009*\"song\" + 0.008*\"johnny\" + 0.008*\"wish\" + 0.008*\"music\" + 0.008*\"on\" + 0.007*\"12\" + 0.007*\"christiana\"')\n",
      "(1, '0.042*\"song\" + 0.038*\"like\" + 0.020*\"good\" + 0.016*\"sing\" + 0.015*\"team\" + 0.012*\"go\" + 0.011*\"voice\" + 0.010*\"really\" + 0.010*\"well\" + 0.010*\"adele\" + 0.009*\"performance\" + 0.009*\"think\"')\n",
      "(2, '0.029*\"like\" + 0.015*\"voice\" + 0.013*\"performance\" + 0.012*\"great\" + 0.011*\"well\" + 0.011*\"make\" + 0.009*\"im\" + 0.008*\"good\" + 0.007*\"kelsea\" + 0.007*\"get\" + 0.007*\"him\" + 0.007*\"true\"')\n",
      " \n",
      "Christiana_Danielle\n",
      " \n",
      "(0, '0.025*\"like\" + 0.020*\"alicia\" + 0.020*\"song\" + 0.016*\"christiana\" + 0.010*\"get\" + 0.010*\"really\" + 0.010*\"voice\" + 0.009*\"make\" + 0.009*\"win\" + 0.009*\"im\" + 0.009*\"terrence\" + 0.008*\"performance\"')\n",
      "(1, '0.027*\"like\" + 0.014*\"christiana\" + 0.013*\"voice\" + 0.013*\"performance\" + 0.013*\"best\" + 0.009*\"one\" + 0.009*\"im\" + 0.008*\"great\" + 0.008*\"good\" + 0.007*\"her\" + 0.006*\"it\" + 0.006*\"yes\"')\n",
      "(2, '0.013*\"show\" + 0.011*\"well\" + 0.011*\"good\" + 0.011*\"song\" + 0.009*\"want\" + 0.009*\"queen\" + 0.008*\"jackie\" + 0.008*\"go\" + 0.007*\"voice\" + 0.006*\"singer\" + 0.006*\"performance\" + 0.006*\"get\"')\n",
      " \n",
      "Kelsea_Johnson\n",
      " \n",
      "(0, '0.048*\"like\" + 0.020*\"voice\" + 0.018*\"song\" + 0.018*\"hill\" + 0.015*\"lauryn\" + 0.012*\"alicia\" + 0.011*\"good\" + 0.011*\"one\" + 0.010*\"well\" + 0.008*\"artist\" + 0.008*\"performance\" + 0.008*\"really\"')\n",
      "(1, '0.090*\"kelsea\" + 0.020*\"make\" + 0.015*\"christiana\" + 0.014*\"top\" + 0.012*\"voice\" + 0.011*\"like\" + 0.011*\"jackie\" + 0.011*\"12\" + 0.011*\"team\" + 0.009*\"would\" + 0.008*\"alicia\" + 0.007*\"think\"')\n",
      "(2, '0.016*\"show\" + 0.012*\"make\" + 0.012*\"voice\" + 0.012*\"song\" + 0.011*\"hope\" + 0.010*\"go\" + 0.009*\"alicia\" + 0.008*\"vote\" + 0.007*\"would\" + 0.007*\"team\" + 0.007*\"christiana\" + 0.007*\"choice\"')\n",
      " \n",
      "Terrence_Cunningham\n",
      " \n",
      "(0, '0.020*\"like\" + 0.013*\"get\" + 0.010*\"im\" + 0.010*\"voice\" + 0.010*\"piano\" + 0.009*\"well\" + 0.008*\"make\" + 0.008*\"performance\" + 0.007*\"he\" + 0.007*\"show\" + 0.007*\"sing\" + 0.006*\"something\"')\n",
      "(1, '0.016*\"like\" + 0.013*\"get\" + 0.012*\"song\" + 0.012*\"terrence\" + 0.011*\"performance\" + 0.009*\"he\" + 0.009*\"one\" + 0.009*\"people\" + 0.008*\"artist\" + 0.008*\"piano\" + 0.008*\"good\" + 0.008*\"alicia\"')\n",
      "(2, '0.020*\"like\" + 0.011*\"dislike\" + 0.011*\"piano\" + 0.009*\"he\" + 0.008*\"go\" + 0.008*\"see\" + 0.007*\"voice\" + 0.007*\"get\" + 0.007*\"show\" + 0.007*\"know\" + 0.007*\"time\" + 0.007*\"one\"')\n",
      " \n",
      "Drew_Cole\n",
      " \n",
      "(0, '0.019*\"well\" + 0.016*\"country\" + 0.014*\"jackie\" + 0.012*\"go\" + 0.011*\"voice\" + 0.010*\"one\" + 0.008*\"favorite\" + 0.008*\"austin\" + 0.007*\"yeah\" + 0.006*\"sing\" + 0.006*\"two\" + 0.006*\"show\"')\n",
      "(1, '0.026*\"adam\" + 0.020*\"song\" + 0.020*\"go\" + 0.016*\"draw\" + 0.015*\"voice\" + 0.014*\"really\" + 0.013*\"country\" + 0.010*\"make\" + 0.010*\"drew\" + 0.008*\"jackie\" + 0.008*\"always\" + 0.008*\"favorite\"')\n",
      "(2, '0.019*\"like\" + 0.011*\"still\" + 0.010*\"drew\" + 0.009*\"top\" + 0.009*\"jackie\" + 0.009*\"season\" + 0.008*\"it\" + 0.008*\"make\" + 0.007*\"sharane\" + 0.007*\"im\" + 0.007*\"team\" + 0.007*\"he\"')\n",
      " \n",
      "Jackie_Verna\n",
      " \n",
      "(0, '0.021*\"adam\" + 0.016*\"think\" + 0.014*\"like\" + 0.014*\"jackie\" + 0.012*\"reid\" + 0.012*\"make\" + 0.011*\"season\" + 0.011*\"go\" + 0.011*\"performance\" + 0.010*\"singer\" + 0.010*\"well\" + 0.010*\"last\"')\n",
      "(1, '0.032*\"jackie\" + 0.018*\"like\" + 0.015*\"country\" + 0.014*\"adam\" + 0.013*\"song\" + 0.012*\"mia\" + 0.011*\"choice\" + 0.011*\"im\" + 0.010*\"reid\" + 0.010*\"good\" + 0.010*\"well\" + 0.010*\"get\"')\n",
      "(2, '0.016*\"jackie\" + 0.014*\"like\" + 0.014*\"mia\" + 0.012*\"song\" + 0.010*\"adam\" + 0.009*\"sing\" + 0.009*\"go\" + 0.009*\"make\" + 0.008*\"say\" + 0.008*\"voice\" + 0.008*\"one\" + 0.008*\"even\"')\n",
      " \n",
      "Mia_Boostrom\n",
      " \n",
      "(0, '0.010*\"good\" + 0.009*\"singer\" + 0.008*\"top\" + 0.008*\"12\" + 0.008*\"mia\" + 0.008*\"people\" + 0.008*\"jackie\" + 0.007*\"show\" + 0.007*\"pryor\" + 0.006*\"one\" + 0.006*\"note\" + 0.006*\"que\"')\n",
      "(1, '0.029*\"like\" + 0.018*\"mia\" + 0.014*\"performance\" + 0.014*\"sad\" + 0.010*\"im\" + 0.010*\"song\" + 0.010*\"sing\" + 0.009*\"adam\" + 0.009*\"jackie\" + 0.008*\"bad\" + 0.007*\"voice\" + 0.007*\"brynn\"')\n",
      "(2, '0.021*\"like\" + 0.021*\"song\" + 0.016*\"mia\" + 0.013*\"performance\" + 0.012*\"voice\" + 0.011*\"bad\" + 0.010*\"go\" + 0.010*\"great\" + 0.010*\"it\" + 0.009*\"make\" + 0.009*\"one\" + 0.008*\"singer\"')\n",
      " \n",
      "Reid_Umstattd\n",
      " \n",
      "(0, '0.019*\"reid\" + 0.018*\"season\" + 0.016*\"voice\" + 0.014*\"deserve\" + 0.013*\"well\" + 0.011*\"agree\" + 0.010*\"jackie\" + 0.010*\"guy\" + 0.009*\"go\" + 0.009*\"top\" + 0.009*\"britton\" + 0.009*\"wilkes\"')\n",
      "(1, '0.054*\"adam\" + 0.014*\"reid\" + 0.013*\"jackie\" + 0.012*\"good\" + 0.011*\"get\" + 0.011*\"actually\" + 0.011*\"really\" + 0.010*\"like\" + 0.009*\"save\" + 0.009*\"go\" + 0.009*\"rayshun\" + 0.008*\"im\"')\n",
      "(2, '0.019*\"performance\" + 0.017*\"voice\" + 0.015*\"go\" + 0.015*\"twitter\" + 0.015*\"reid\" + 0.015*\"vote\" + 0.010*\"get\" + 0.010*\"great\" + 0.009*\"im\" + 0.009*\"like\" + 0.008*\"hope\" + 0.008*\"watch\"')\n",
      " \n",
      "Austin_Giorgio\n",
      " \n",
      "(0, '0.033*\"like\" + 0.019*\"buble\" + 0.017*\"sound\" + 0.016*\"song\" + 0.016*\"austin\" + 0.013*\"michael\" + 0.010*\"well\" + 0.009*\"im\" + 0.008*\"sing\" + 0.008*\"good\" + 0.008*\"go\" + 0.007*\"it\"')\n",
      "(1, '0.039*\"like\" + 0.035*\"song\" + 0.018*\"good\" + 0.016*\"choice\" + 0.015*\"think\" + 0.009*\"he\" + 0.008*\"style\" + 0.008*\"go\" + 0.008*\"show\" + 0.008*\"really\" + 0.007*\"austin\" + 0.007*\"turtleneck\"')\n",
      "(2, '0.040*\"song\" + 0.013*\"sing\" + 0.011*\"austin\" + 0.011*\"make\" + 0.010*\"performance\" + 0.010*\"bad\" + 0.009*\"artist\" + 0.008*\"choice\" + 0.008*\"round\" + 0.008*\"voice\" + 0.008*\"team\" + 0.007*\"get\"')\n",
      " \n",
      "Gary_Edwards\n",
      " \n",
      "(0, '0.042*\"song\" + 0.015*\"america\" + 0.014*\"like\" + 0.012*\"good\" + 0.010*\"gary\" + 0.010*\"think\" + 0.010*\"american\" + 0.009*\"sing\" + 0.009*\"message\" + 0.008*\"get\" + 0.008*\"make\" + 0.008*\"say\"')\n",
      "(1, '0.016*\"sing\" + 0.014*\"song\" + 0.011*\"well\" + 0.010*\"national\" + 0.009*\"anthem\" + 0.007*\"performance\" + 0.007*\"vote\" + 0.007*\"one\" + 0.007*\"beautiful\" + 0.006*\"lmao\" + 0.006*\"get\" + 0.006*\"confuse\"')\n",
      "(2, '0.028*\"country\" + 0.023*\"song\" + 0.019*\"like\" + 0.012*\"world\" + 0.012*\"america\" + 0.011*\"get\" + 0.011*\"people\" + 0.009*\"take\" + 0.008*\"right\" + 0.008*\"choice\" + 0.008*\"one\" + 0.008*\"voice\"')\n",
      " \n",
      "Spensha_Baker\n",
      " \n",
      "(0, '0.035*\"song\" + 0.018*\"spensha\" + 0.018*\"good\" + 0.016*\"think\" + 0.016*\"choice\" + 0.013*\"like\" + 0.013*\"would\" + 0.011*\"wilkes\" + 0.011*\"country\" + 0.010*\"go\" + 0.009*\"make\" + 0.008*\"voice\"')\n",
      "(1, '0.020*\"like\" + 0.014*\"blake\" + 0.013*\"spensha\" + 0.012*\"voice\" + 0.012*\"country\" + 0.012*\"her\" + 0.011*\"carrie\" + 0.010*\"get\" + 0.009*\"song\" + 0.009*\"well\" + 0.009*\"save\" + 0.008*\"sound\"')\n",
      "(2, '0.036*\"like\" + 0.021*\"voice\" + 0.019*\"country\" + 0.012*\"sing\" + 0.012*\"song\" + 0.010*\"sound\" + 0.010*\"feel\" + 0.008*\"u\" + 0.008*\"genre\" + 0.008*\"get\" + 0.008*\"gospel\" + 0.008*\"it\"')\n",
      " \n",
      "Wilkes\n",
      " \n",
      "(0, '0.025*\"wilkes\" + 0.015*\"make\" + 0.014*\"song\" + 0.013*\"performance\" + 0.012*\"blake\" + 0.012*\"he\" + 0.011*\"get\" + 0.010*\"mic\" + 0.009*\"save\" + 0.009*\"like\" + 0.009*\"top\" + 0.008*\"im\"')\n",
      "(1, '0.030*\"like\" + 0.030*\"wilkes\" + 0.015*\"mic\" + 0.013*\"drop\" + 0.008*\"go\" + 0.008*\"make\" + 0.008*\"im\" + 0.007*\"get\" + 0.007*\"way\" + 0.006*\"on\" + 0.006*\"sound\" + 0.006*\"good\"')\n",
      "(2, '0.015*\"song\" + 0.014*\"think\" + 0.014*\"mic\" + 0.014*\"like\" + 0.014*\"go\" + 0.013*\"voice\" + 0.013*\"wilkes\" + 0.012*\"good\" + 0.011*\"performance\" + 0.009*\"he\" + 0.009*\"save\" + 0.009*\"best\"')\n",
      " \n",
      "Alexa_Cappelli\n",
      " \n",
      "(0, '0.022*\"kelly\" + 0.016*\"alexa\" + 0.014*\"team\" + 0.014*\"song\" + 0.013*\"know\" + 0.012*\"choice\" + 0.012*\"dr\" + 0.012*\"go\" + 0.011*\"robbed\" + 0.011*\"get\" + 0.008*\"kaleb\" + 0.007*\"really\"')\n",
      "(1, '0.022*\"like\" + 0.014*\"song\" + 0.012*\"move\" + 0.011*\"alexa\" + 0.011*\"voice\" + 0.011*\"well\" + 0.011*\"people\" + 0.011*\"good\" + 0.010*\"make\" + 0.010*\"kaleb\" + 0.008*\"im\" + 0.008*\"season\"')\n",
      "(2, '0.033*\"song\" + 0.026*\"rob\" + 0.026*\"like\" + 0.013*\"alexa\" + 0.010*\"didnt\" + 0.009*\"choice\" + 0.009*\"see\" + 0.009*\"performance\" + 0.009*\"good\" + 0.008*\"show\" + 0.008*\"get\" + 0.007*\"would\"')\n",
      " \n",
      "D.R._King\n",
      " \n",
      "(0, '0.028*\"dr\" + 0.026*\"like\" + 0.016*\"make\" + 0.016*\"kelly\" + 0.015*\"king\" + 0.013*\"song\" + 0.011*\"top\" + 0.011*\"12\" + 0.010*\"choice\" + 0.010*\"happy\" + 0.008*\"season\" + 0.008*\"team\"')\n",
      "(1, '0.017*\"like\" + 0.016*\"get\" + 0.016*\"save\" + 0.015*\"dr\" + 0.014*\"go\" + 0.012*\"deserve\" + 0.011*\"song\" + 0.009*\"kelly\" + 0.009*\"think\" + 0.009*\"vote\" + 0.009*\"good\" + 0.008*\"cant\"')\n",
      "(2, '0.013*\"dr\" + 0.013*\"like\" + 0.012*\"see\" + 0.012*\"sing\" + 0.009*\"good\" + 0.009*\"one\" + 0.009*\"you\" + 0.009*\"really\" + 0.009*\"probably\" + 0.009*\"song\" + 0.008*\"performance\" + 0.007*\"get\"')\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dylan_Hartigan\n",
      " \n",
      "(0, '0.024*\"voice\" + 0.015*\"like\" + 0.013*\"go\" + 0.010*\"song\" + 0.010*\"look\" + 0.009*\"im\" + 0.009*\"really\" + 0.008*\"good\" + 0.008*\"tom\" + 0.008*\"petty\" + 0.008*\"dylan\" + 0.008*\"bad\"')\n",
      "(1, '0.015*\"well\" + 0.014*\"vote\" + 0.013*\"go\" + 0.010*\"get\" + 0.010*\"good\" + 0.009*\"see\" + 0.009*\"sad\" + 0.009*\"he\" + 0.009*\"choice\" + 0.009*\"kaleb\" + 0.008*\"sing\" + 0.008*\"song\"')\n",
      "(2, '0.027*\"dylan\" + 0.020*\"like\" + 0.012*\"get\" + 0.012*\"im\" + 0.012*\"vote\" + 0.012*\"make\" + 0.011*\"dr\" + 0.010*\"it\" + 0.010*\"kaleb\" + 0.008*\"top\" + 0.008*\"he\" + 0.008*\"america\"')\n",
      " \n",
      "Tish_Haynes_Keys\n",
      " \n",
      "(0, '0.024*\"song\" + 0.019*\"kyla\" + 0.015*\"get\" + 0.015*\"go\" + 0.014*\"good\" + 0.013*\"best\" + 0.010*\"bad\" + 0.010*\"singer\" + 0.010*\"much\" + 0.009*\"tish\" + 0.009*\"like\" + 0.008*\"key\"')\n",
      "(1, '0.020*\"like\" + 0.016*\"sing\" + 0.016*\"good\" + 0.011*\"make\" + 0.011*\"country\" + 0.011*\"voice\" + 0.010*\"singer\" + 0.009*\"america\" + 0.009*\"show\" + 0.008*\"win\" + 0.008*\"im\" + 0.008*\"big\"')\n",
      "(2, '0.016*\"jackie\" + 0.015*\"top\" + 0.015*\"kyla\" + 0.013*\"12\" + 0.012*\"go\" + 0.011*\"team\" + 0.011*\"kaleb\" + 0.011*\"christiana\" + 0.010*\"it\" + 0.010*\"kelly\" + 0.009*\"season\" + 0.008*\"jade\"')\n",
      " \n"
     ]
    }
   ],
   "source": [
    "Adam_Levine=['Drew_Cole','Jackie_Verna','Mia_Boostrom','Reid_Umstattd']\n",
    "Alicia_Keys=['Johnny_Bliss','Christiana_Danielle','Kelsea_Johnson','Terrence_Cunningham']\n",
    "Blake_Shelton=['Austin_Giorgio','Gary_Edwards','Spensha_Baker','Wilkes']\n",
    "Kelly_Clarkson=['Alexa_Cappelli','D.R._King','Dylan_Hartigan','Tish_Haynes_Keys']\n",
    "experts={'Alicia_Keys':Alicia_Keys,'Adam_Levine':Adam_Levine,'Blake_Shelton':Blake_Shelton,'Kelly_Clarkson':Kelly_Clarkson}\n",
    "for key,value in experts.items():\n",
    "   \n",
    "        \n",
    "    for i in range(len(value)):\n",
    "        data=pd.read_csv(r'/Users/shanqin/Desktop/TextaAnalysis/Final_Project/data_by_mentor/{}/{}.csv'.format(key,value[i]))\n",
    "        stop = set(stopwords.words('english'))\n",
    "        exclude = set(string.punctuation) \n",
    "        lemma = WordNetLemmatizer()\n",
    "        frames = [data['commentText'][pd.notna(data['commentText'])], \\\n",
    "                  data['replies.commentText'][pd.notna(data['replies.commentText'])]]\n",
    "        datanona=pd.concat(frames)\n",
    "        \n",
    "        def clean(doc):\n",
    "            stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "            punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "            other_punc_free=''.join(ch for ch in punc_free if ch!='’')\n",
    "            normalized = \" \".join(lemma.lemmatize(word) for word in other_punc_free.split())\n",
    "            replacelove=normalized.replace('love','like')\n",
    "            pos_tagging_without_stopwords=nltk.pos_tag(replacelove.split())\n",
    "            def get_wordnet_pos(treebank_tag):\n",
    "                if treebank_tag.startswith('J'):\n",
    "                    return wordnet.ADJ\n",
    "                elif treebank_tag.startswith('V'):\n",
    "                    return wordnet.VERB\n",
    "                elif treebank_tag.startswith('N'):\n",
    "                    return wordnet.NOUN\n",
    "                elif treebank_tag.startswith('R'):\n",
    "                    return wordnet.ADV\n",
    "                else:\n",
    "                    return None\n",
    "            l=[]\n",
    "            for word, tag in pos_tagging_without_stopwords:\n",
    "                wntag = get_wordnet_pos(tag)\n",
    "                if wntag is None:\n",
    "                    lemma1 = lemma.lemmatize(word)\n",
    "                    l.append(lemma1)\n",
    "                else:\n",
    "                    lemma1 = lemma.lemmatize(word, pos=wntag) \n",
    "                    l.append(lemma1)\n",
    "            return ' '.join(l)\n",
    "        datacom=datanona.map(clean)\n",
    "        datacomtoken=datacom.map(word_tokenize)\n",
    "        id2word = corpora.Dictionary(datacomtoken)\n",
    "        corpus=[id2word.doc2bow(text) for text in datacomtoken]\n",
    "\n",
    "        NUM_TOPICS = 3\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15,random_state=1)\n",
    "        topics = ldamodel.print_topics(num_words=12)\n",
    "        print (value[i])\n",
    "        print(' ')\n",
    "        #vis_data = gensimvis.prepare(ldamodel, corpus, id2word)\n",
    "        #pyLDAvis.display(vis_data)\n",
    "        for topic in topics:\n",
    "            print(topic)\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all the comments(3 topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanqin/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"jackie\" + 0.017*\"top\" + 0.016*\"team\" + 0.015*\"kaleb\" + 0.013*\"christiana\" + 0.013*\"dr\" + 0.013*\"kelly\" + 0.011*\"alicia\" + 0.011*\"12\" + 0.011*\"dylan\" + 0.011*\"agree\" + 0.011*\"singer\"')\n",
      "(1, '0.029*\"song\" + 0.022*\"like\" + 0.016*\"good\" + 0.015*\"voice\" + 0.012*\"get\" + 0.012*\"performance\" + 0.011*\"think\" + 0.010*\"choice\" + 0.010*\"make\" + 0.010*\"go\" + 0.008*\"bad\" + 0.008*\"well\"')\n",
      "(2, '0.019*\"like\" + 0.015*\"country\" + 0.010*\"na\" + 0.010*\"win\" + 0.010*\"sing\" + 0.009*\"yes\" + 0.009*\"voice\" + 0.008*\"guy\" + 0.007*\"gon\" + 0.007*\"make\" + 0.006*\"you\" + 0.006*\"hate\"')\n",
      " \n"
     ]
    }
   ],
   "source": [
    "Adam_Levine=['Drew_Cole','Jackie_Verna','Mia_Boostrom','Reid_Umstattd']\n",
    "Alicia_Keys=['Johnny_Bliss','Christiana_Danielle','Kelsea_Johnson','Terrence_Cunningham']\n",
    "Blake_Shelton=['Austin_Giorgio','Gary_Edwards','Spensha_Baker','Wilkes']\n",
    "Kelly_Clarkson=['Alexa_Cappelli','D.R._King','Dylan_Hartigan','Tish_Haynes_Keys']\n",
    "experts={'Alicia_Keys':Alicia_Keys,'Adam_Levine':Adam_Levine,'Blake_Shelton':Blake_Shelton,'Kelly_Clarkson':Kelly_Clarkson}\n",
    "allthecsv=[]\n",
    "for key,value in experts.items():\n",
    "    for i in range(len(value)):\n",
    "        value[i]=pd.read_csv(r'/Users/shanqin/Desktop/TextaAnalysis/Final_Project/data_by_mentor/{}/{}.csv'.format(key,value[i]))\n",
    "        allthecsv.append(value[i])\n",
    "data=pd.concat(allthecsv)\n",
    "frames = [data['commentText'][pd.notna(data['commentText'])],data['replies.commentText'][pd.notna(data['replies.commentText'])]]\n",
    "datanona=pd.concat(frames)\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    other_punc_free=''.join(ch for ch in punc_free if ch!='’')\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in other_punc_free.split())\n",
    "    replacelove=normalized.replace('love','like')\n",
    "    pos_tagging_without_stopwords=nltk.pos_tag(replacelove.split())\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    l=[]\n",
    "    for word, tag in pos_tagging_without_stopwords:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:\n",
    "            lemma1 = lemma.lemmatize(word)\n",
    "            l.append(lemma1)\n",
    "        else:\n",
    "            lemma1 = lemma.lemmatize(word, pos=wntag) \n",
    "            l.append(lemma1)\n",
    "    return ' '.join(l)\n",
    "datacom=datanona.map(clean)\n",
    "datacomtoken=datacom.map(word_tokenize)\n",
    "id2word = corpora.Dictionary(datacomtoken)\n",
    "corpus=[id2word.doc2bow(text) for text in datacomtoken]\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15,random_state=1)\n",
    "topics = ldamodel.print_topics(num_words=12)\n",
    "print(' ')\n",
    "vis_data = gensimvis.prepare(ldamodel, corpus, id2word)\n",
    "pyLDAvis.save_html(vis_data, 'all.html')\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## winner's comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "(0, '0.024*\"song\" + 0.022*\"like\" + 0.016*\"voice\" + 0.011*\"alicia\" + 0.009*\"make\" + 0.009*\"christiana\" + 0.008*\"country\" + 0.007*\"sing\" + 0.007*\"save\" + 0.007*\"girl\" + 0.007*\"team\" + 0.007*\"sound\"')\n",
      "(1, '0.020*\"jackie\" + 0.014*\"go\" + 0.012*\"adam\" + 0.010*\"good\" + 0.010*\"like\" + 0.009*\"mia\" + 0.009*\"performance\" + 0.009*\"well\" + 0.008*\"im\" + 0.008*\"one\" + 0.008*\"reid\" + 0.008*\"singer\"')\n",
      "(2, '0.030*\"like\" + 0.011*\"get\" + 0.011*\"performance\" + 0.010*\"top\" + 0.010*\"best\" + 0.010*\"make\" + 0.009*\"song\" + 0.009*\"im\" + 0.008*\"well\" + 0.008*\"good\" + 0.008*\"think\" + 0.007*\"jackie\"')\n",
      " \n"
     ]
    }
   ],
   "source": [
    "Adam_Levine=['Jackie_Verna']\n",
    "Alicia_Keys=['Christiana_Danielle']\n",
    "Blake_Shelton=['Spensha_Baker']\n",
    "Kelly_Clarkson=['D.R._King']\n",
    "experts={'Alicia_Keys':Alicia_Keys,'Adam_Levine':Adam_Levine,'Blake_Shelton':Blake_Shelton,'Kelly_Clarkson':Kelly_Clarkson}\n",
    "allthecsv=[]\n",
    "for key,value in experts.items():\n",
    "    for i in range(len(value)):\n",
    "        value[i]=pd.read_csv(r'/Users/shanqin/Desktop/TextaAnalysis/Final_Project/data_by_mentor/{}/{}.csv'.format(key,value[i]))\n",
    "        allthecsv.append(value[i])\n",
    "data=pd.concat(allthecsv)\n",
    "frames = [data['commentText'][pd.notna(data['commentText'])],data['replies.commentText'][pd.notna(data['replies.commentText'])]]\n",
    "datanona=pd.concat(frames)\n",
    "stop = set(stopwords.words('english'))\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    other_punc_free=''.join(ch for ch in punc_free if ch!='’')\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in other_punc_free.split())\n",
    "    replacelove=normalized.replace('love','like')\n",
    "    pos_tagging_without_stopwords=nltk.pos_tag(replacelove.split())\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    l=[]\n",
    "    for word, tag in pos_tagging_without_stopwords:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:\n",
    "            lemma1 = lemma.lemmatize(word)\n",
    "            l.append(lemma1)\n",
    "        else:\n",
    "            lemma1 = lemma.lemmatize(word, pos=wntag) \n",
    "            l.append(lemma1)\n",
    "    return ' '.join(l)\n",
    "datacom=datanona.map(clean)\n",
    "datacomtoken=datacom.map(word_tokenize)\n",
    "id2word = corpora.Dictionary(datacomtoken)\n",
    "corpus=[id2word.doc2bow(text) for text in datacomtoken]\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15,random_state=1)\n",
    "topics = ldamodel.print_topics(num_words=12)\n",
    "print(' ')\n",
    "vis_data = gensimvis.prepare(ldamodel, corpus, id2word)\n",
    "pyLDAvis.save_html(vis_data, 'winner.html')\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## losers' comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "(0, '0.015*\"go\" + 0.014*\"team\" + 0.013*\"vote\" + 0.013*\"jackie\" + 0.012*\"top\" + 0.010*\"wilkes\" + 0.009*\"12\" + 0.009*\"think\" + 0.009*\"make\" + 0.009*\"kelsea\" + 0.008*\"performance\" + 0.008*\"best\"')\n",
      "(1, '0.031*\"song\" + 0.023*\"like\" + 0.011*\"get\" + 0.010*\"voice\" + 0.010*\"one\" + 0.010*\"sing\" + 0.010*\"make\" + 0.009*\"good\" + 0.009*\"im\" + 0.008*\"it\" + 0.008*\"show\" + 0.007*\"he\"')\n",
      "(2, '0.024*\"like\" + 0.012*\"voice\" + 0.010*\"good\" + 0.008*\"lol\" + 0.006*\"really\" + 0.006*\"sound\" + 0.005*\"think\" + 0.005*\"well\" + 0.004*\"overrate\" + 0.004*\"que\" + 0.004*\"shes\" + 0.004*\"wow\"')\n",
      " \n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "Adam_Levine=['Drew_Cole','Mia_Boostrom','Reid_Umstattd']\n",
    "Alicia_Keys=['Johnny_Bliss','Kelsea_Johnson','Terrence_Cunningham']\n",
    "Blake_Shelton=['Austin_Giorgio','Gary_Edwards','Wilkes']\n",
    "Kelly_Clarkson=['Alexa_Cappelli','Dylan_Hartigan','Tish_Haynes_Keys']\n",
    "experts={'Alicia_Keys':Alicia_Keys,'Adam_Levine':Adam_Levine,'Blake_Shelton':Blake_Shelton,'Kelly_Clarkson':Kelly_Clarkson}\n",
    "allthecsv=[]\n",
    "for key,value in experts.items():\n",
    "    for i in range(len(value)):\n",
    "        value[i]=pd.read_csv(r'/Users/shanqin/Desktop/TextaAnalysis/Final_Project/data_by_mentor/{}/{}.csv'.format(key,value[i]))\n",
    "        allthecsv.append(value[i])\n",
    "data=pd.concat(allthecsv)\n",
    "frames = [data['commentText'][pd.notna(data['commentText'])],data['replies.commentText'][pd.notna(data['replies.commentText'])]]\n",
    "datanona=pd.concat(frames)\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    other_punc_free=''.join(ch for ch in punc_free if ch!='’')\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in other_punc_free.split())\n",
    "    replacelove=normalized.replace('love','like')\n",
    "    pos_tagging_without_stopwords=nltk.pos_tag(replacelove.split())\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    l=[]\n",
    "    for word, tag in pos_tagging_without_stopwords:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:\n",
    "            lemma1 = lemma.lemmatize(word)\n",
    "            l.append(lemma1)\n",
    "        else:\n",
    "            lemma1 = lemma.lemmatize(word, pos=wntag) \n",
    "            l.append(lemma1)\n",
    "    return ' '.join(l)\n",
    "datacom=datanona.map(clean)\n",
    "datacomtoken=datacom.map(word_tokenize)\n",
    "id2word = corpora.Dictionary(datacomtoken)\n",
    "corpus=[id2word.doc2bow(text) for text in datacomtoken]\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=id2word, passes=15,random_state=1)\n",
    "topics = ldamodel.print_topics(num_words=12)\n",
    "print(' ')\n",
    "vis_data = gensimvis.prepare(ldamodel, corpus, id2word)\n",
    "pyLDAvis.save_html(vis_data, 'loser.html')\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
